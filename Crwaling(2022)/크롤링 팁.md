크롤링의 자잘자잘한 팁
---

네이버는 봇을 잡기위해서 빠르게 로그인을 하게되면(셀레니움)같은 경우 자동방지 문자를 띄운다. 

또한 너무 빠르게하면 문제가 있으니 중간 중간 마다 

```
import time
time.sleep(1~2)
```
해서 쉼을 주면 좋다.

그럴땐 해결 방법이 몇가지가 있다.

1. 셀레니움으로 send_keys(네이버아이디)보단

 send_keys(Keys.CONTROL,'v')형태로 할 경우 우회가 가능하다.\
Keys.ENTER 도 가능

- 복사하는 방법 같은 경우 
```
import pyperclip

pyperclip.copy('네이버 아이디')
```

2. 실제 크롬브라우저의 프로필을 chromedriver.exe 오픈할때 복사 붙여 넣기한다. 

```
from selenium.webdriver.chrome.options import Options

...
driver = webdriver.Chrome('chromedriver.exe',chrome_option=옵션)

옵션 = webdriver.ChromeOptions()

// 크롬브라우저로 쓰던 계정 정보 복붙
// chrome://version 에서 확인 가능함.(프로필 경로)
옵션.add_argument(r'user-data-dir=<여기에 프로필 경로>')
```

3. Headers 정보 복붙

- 해당 내용은 2021 크롤링 공부할때 다뤘음.

- HEADER를 개발자도구를 참고해서 따로 만들어서 넣어주는 방법

-----

네이버 블로그 봇 팁
---
 
1. 글을 퍼올경우 복붙한 내용에 있는 내용을 \n 으로 바꿔준다.(replace사용)


2. 이미지 입력 하는법

> 1. <input type="file"> 이라는 요소를 ctrl+f로 찾아서 send_keys()로 내 하드에 있는 이미지 경로를 입력

> 2.네이버블로그는 간편하게도 다른 곳에 올려진 이미지 주소를 그대로 복사해와서

글내용자리에 ctrl+v 하시면 그대로 붙여넣기 가능

------

AMAZONE

아마존 같은 경우 requests를 사용해도 정보가 제대로 나오지 않는다.

이유는 해당 API를 사서 쓰라고 하는 것 이기 때문에 크롤링 보단 API를 사서 쓰는게 더 좋을 듯 하다. 

하지만 봇으로 접속을 했다는 사실을 어떻게 알까?

1번. 개발자 도구 Network- 새로고침 - 맨위 뜨는 파일 클림 - Request Headers - user-agent 를 확인하면 GET요청을 할때 접속한 사람의 정보가 뜬다.

그래서 그 부분에 대하여 다른 정보를 채워주는 것이다.

```
import requests

헤더스 = {
  'User-Agent': '1번 항목내용'
}

r = requests.get('https://www.amazon.com/s?k=monitor&ref=nb_sb_noss_2', headers=헤더스)
print(r.content)
print(r.status_code)
```

또한 이것만으로는 부족하다. 쿠기에도 정보가 저장되야하는데 파이썬으로 접속할 경우 쿠기가 없을 수도 있다.

그부분에 대하여 

1. headers 정보에 cookies 내용을 전부 dict 형태로 바꿔준다.

2.
```
r = requests.get('https://www.amazon.com/s?k=monitor&ref=nb_sb_noss_2', headers=헤더스, cookies=쿠키)
```

또한 에러가 발생하여 크롤링이 중된 되는 경우를 try: except:문으로 처리를 해줄 수 있다.

```
if r.status_code == 200 : 
  crwal
else : 
  print("NO")
```