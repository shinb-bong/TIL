{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.머신러닝 개념.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3Qe60vQaZ/JPdH0C7abOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinb-bong/TIL/blob/main/1_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EA%B0%9C%EB%85%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "머신러닝의 종류\n",
        "\n",
        "- Supervised Learning: 데이터에 정답이 있고 정답 예측 모델을 만들때\n",
        "\n",
        "-  Unsupervised Learning: 데이터에 정답이 없고 컴퓨터가 알아서 분류하길 원할때\n",
        "\n",
        "- Reinforcement Learning: 강화 학습 \n",
        "\n",
        "딥러닝 \n",
        "\n",
        "- perceptron에 중간 hidden layer를 설치하여 Neural Network를 만들어서 원하는 결과를 나오게 하는 것\n",
        "\n",
        "Neural Network \n",
        "\n",
        "hideen 레이어 가 있을때 예측 결과 계산하는법\n",
        "\n",
        "- 오차를 최소화하는 방향으로 w값을 찾아라 라고 명령을 내려 줘야함."
      ],
      "metadata": {
        "id": "GCxaRfUb9BuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function(손실 함수) \n",
        "\n",
        "- 총 오차들을 계산할때 음수 오차나 그런것은 그저 더하기를 하는 것은 오류를 발생 시킬 수 있다.\n",
        "\n",
        "- 그래서 오차를 구하는 수식을 사용한다. \n",
        "\n",
        "*여기서 잠깐!*\n",
        "\n",
        "활성함수가 없으면 히든 레이어를 쓰나 마나 치환을 했을 경우 같은 결과 값이 나옴\n",
        "\n",
        "Activation Function(활성함수)란?\n",
        "\n",
        "- 바로바로 나오는 결과값을 그다음 노드 계산할때 사용하지 말고 넣었다가 활성함수를 사용하여 다른 대변할 수 있는 값으로 내보내준다.\n",
        "\n",
        "- 비 선형적인 예측을 하고 싶을때 사용\n",
        "\n",
        "- 출력층은 출력이기 때문에 활성함수를 안하는 경우가 있다. \n",
        "\n",
        "- ex) sigmoid , hyperbolic tangent, Rectified Linear Units"
      ],
      "metadata": {
        "id": "TE_Ro4oMAh2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent(경사 하강법)\n",
        "\n",
        "- 처음엔 랜덤으로 찍는다. 이걸로 총 손실 값을 구해지는데 손실을 최대한 적게 만들기 위해서 조정하는 방법이다.\n",
        "\n",
        "- 최적으로 구하기 위해선 현재 가중치의 접선의 기울기를 현재 가중치에서 뺀다.\n",
        "\n",
        "- 위의 사안을 최저가 나올때까지 반복한다. \n",
        "\n",
        "머신 러닝은 결국 손실을 최소화 하는 w값을 찾게 시키는 것이고 그 방법이 경사 하강인 것이다.\n",
        "\n",
        "1. w값들 랜덤\n",
        "2. w값 바탕으로 총 손실 E 계산\n",
        "3. 경사하강으로 새로운 w값 업데이트\n",
        "3번 반복\n",
        "\n",
        "이때 Learning rate(a)가 필요한데 접선의 기울기가 0 이 되는 부분이 여러개 일 경우 하나만 찾고 멈출 가능성이 있다. 그래서 a만큼 곱해진 기울기를 빼서 조정을 한다.\n",
        "\n",
        "learning rate optimizer\n",
        "\n",
        "- momentum: 가속도 유지\n",
        "- AdaGrad: 자주변하는 w작게, 자주변하면 크게\n",
        "등등"
      ],
      "metadata": {
        "id": "ImcIx3c9Cn6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i7TXmftvDDFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9wE1eU_82KF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}